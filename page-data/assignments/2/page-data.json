{"componentChunkName":"component---src-templates-blog-post-js","path":"/assignments/2","result":{"data":{"markdownRemark":{"rawMarkdownBody":"\n# Assignment 2: Learning Machine Learning\n\nIn our third assignment, we will be exploring the field of computational linguistics, otherwise known as **Natural Language Processing**. The goal of this assignment is to have you become familiar with working with reading/writing to files, and working with third party packages. We'll explore these Python ideas through the lens of Data Science and Machine Learning.\n\n## Part 0: Setup\n\nSkim through the assignment and install the relevant packages for this assignment through pip (e.g. [Sci-Kit Learn](https://github.com/scikit-learn/scikit-learn) and [NumPy](https://github.com/numpy/numpy)). Next, download the homework datasets [here](https://github.com/CIS192/homework/raw/master/assignment2/data.zip) (or from the GitHub repository). Finally, download the skeleton code, as well as the report template from the [assignment's GitHub repository](https://github.com/CIS192/homework/tree/master/assignment2).\n\n## Part 1: NLP Basics\n\nFor the first part of the homework you will be implementing a couple of basic NLP tasks in `part1.py`, including raw text analysis with CSV, text tokenizing, and word importance with a score called TF-IDF. The data file `raven.txt` is located in the `data.zip` file, so make sure to unzip it to `/data`! The remaining dataset files will be used for Part 2, so be sure to keep those handy.\n\n**TODO:** Implement the incomplete stubs in `part1.py`.\n\n## Part 2: Classification with Sci-Kit Learn\n\n> Adapted from CIS 530 - Computational Linguistics\n\n### Preamble\n\nThe second part of the homework will be a longer project: building a text classifier. Now that we have seen tokenizing, text cleaning, and word importance with TF-IDF, let's train a text classifier that will be able to classify a word as being simple (e.g. _easy_, _act_, _blue_) or complex (e.g. _ostentatious_, _esoteric_, _aberration_). This is an important step in a larger NLP task to simply texts to make text more readable.\n\nIn the provided code template with provided helper and unimplemented functions, you will need to:\n\n0. Look at the dataset! Try to understand the information that is conveyed to better understand the task.\n1. Implement the machine learning evaluation metric we discussed in class (accuracy).\n1. Perform data pre-processing for our dataset. You will need to parse the provided pre-labeled data in training/test sets, and implement a simple baseline model.\n1. Use the Sci-Kit Learn package to train machine learning models which classif\n   y words as simple or complex.\n\nWe have provided the dataset of labelled words split between training/test sets in (.txt) format. Some notes on the dataset:\n\n1. The training set is disjoint, so a word in `complex_words_training.txt` will not appear in the test set.\n2. Stop-words and proper nouns are already removed, leaving only nouns, verbs, and adjectives.\n3. There are 4,000 training words, and 1,000 testing words.\n4. The relevant columns are WORD (the word to be classified), and LABEL (0 for simple, 1 for complex).\n\nWe have also provided frequencies (a contiguous sequence of 1 item from a given sample of text or speech) from the [Google N-Gram Corpus](https://books.google.com/ngrams/info). This is to provide you another feature for classification. Consider why this extra information is useful for distinguishing between simple and complex words.\n\nBe sure to install `numpy` and `sklearn` before starting.\n\n### Section 0: Data (0 points)\n\nWe have provided the function `load_file` that takes in the file name `data_file` of one of the datasets and returns the words and labels of that dataset. The second provided helper function `load_ngram_counts` loads Google N-Gram counts from our provided file `ngram_counts.txt` as a dictionary of word frequencies.\n\n**TODO:** Inspect these functions, print out what they return and make sure you understand what they're providing before moving on.\n\n### Section 1: Evaluation (5 points)\n\nWe will be implementing **accuracy**, a standard evaluation metric that we discussed in class. We will use this function later in the assignment, so be sure that this function works before moving on.\n\n**TODO:** Implement `get_accuracy`, which should return a value between 0 and 1 that corresponds to the amount of predictions that match the true labels.\n\n### Section 2: Baseline Models (20 points)\n\nIn the following functions, you will implement 3 baseline models. Recall that baseline models are used to benchmark our own machine learning models against.\n\n1. The first baseline model `all_complex` classifies ALL words as complex (think back to the coin-flipping example from class).\n2. The second baseline model `word_length_threshold` uses word length thresholding: if a word is longer than the given threshold, we consider it to be complex, and vice versa.\n3. The third baseline model `word_frequency_threshold` is similar to the second, but we will use frequencies from the Google N-Gram counts dataset as the metric to threshold against.\n\n**TODO:** Implement the three baseline models, and report their accuracies (using the function you implemented earlier).\n\n### Section 3: Machine Learning Models (20 points)\n\nFor our machine learning classifiers, we will use the built-in Naive Bayes and Logistic Regression models from Sci-Kit Learn to train a classifier. Refer to the [Sci-Kit Learn Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html) for more information about the Sci-Kit Learn API. Feel free to experiment with other models for extra credit!\n\nFor features, you'll want to use both word length and word frequency. However, feel free to use any other features that you want! Be sure to document any extra features in `REPORT.md`. Extra credit is available for the inclusion of any other interesting features!\n\nYou can import the relevant models from Sci-Kit Learn using:\n\n```python\nfrom sklearn.naive_bayes import GaussianNB\n```\n\nfor Naive Bayes and\n\n```python\nfrom sklearn.linear_model import LogisticRegression\n```\n\nfor Logistic Regression. To train a classifier, you need two numpy arrays:\n\n1. `X_train`: size (`m` x `n`), where `m` is the number of words and `n` is the number of features for each word.\n2. `Y_train`: size (`m` x `1`) for the labels of each of the `m` words.\n\nSince Sci-Kit Learn classifiers take in `numpy` arrays, always wrap your lists in a `np.array`:\n\n```python\nmy_array = np.array([1, 2, 3, 4, 5])\n```\n\nAs such, you'll want to import `numpy` using:\n\n```python\nimport numpy as np\n```\n\n**TODO:** Implement `logistic_regression` and `naive_bayes`, where you will train the machine learning models and report their accuracies on the testing data.\n\n### Section 4: Report (5 points)\n\n**TODO:** Complete `REPORT.md` with information about your implementations and your accuracies for each section. Write a few comments comparing the performace of your Naive Bayes classifier and your Logistic Regression classifier. Include any details about extra credit you've completed.\n\nBe sure to complete the report in [Markdown](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet) format. Remember to indicate which member worked on which sections for full credit.\n\n## Submission\n\nSubmit all your code and potentially extra data to Gradescope. If you have a partner, YOU MUST MARK THEM AS A COLLABORATOR ON GRADESCOPE. If you fail to do this, you may get a 0 on this assignment.\n\n## Attributions\n\n<small> This homework was adapted from [CIS 530: Computational Linguistics at the University of Pennsylvania](https://computational-linguistics-class.org/) by Arun Kirubarajan and Kevin Sun.</small>\n","html":"<h1>Assignment 2: Learning Machine Learning</h1>\n<p>In our third assignment, we will be exploring the field of computational linguistics, otherwise known as <strong>Natural Language Processing</strong>. The goal of this assignment is to have you become familiar with working with reading/writing to files, and working with third party packages. We'll explore these Python ideas through the lens of Data Science and Machine Learning.</p>\n<h2>Part 0: Setup</h2>\n<p>Skim through the assignment and install the relevant packages for this assignment through pip (e.g. <a href=\"https://github.com/scikit-learn/scikit-learn\">Sci-Kit Learn</a> and <a href=\"https://github.com/numpy/numpy\">NumPy</a>). Next, download the homework datasets <a href=\"https://github.com/CIS192/homework/raw/master/assignment2/data.zip\">here</a> (or from the GitHub repository). Finally, download the skeleton code, as well as the report template from the <a href=\"https://github.com/CIS192/homework/tree/master/assignment2\">assignment's GitHub repository</a>.</p>\n<h2>Part 1: NLP Basics</h2>\n<p>For the first part of the homework you will be implementing a couple of basic NLP tasks in <code class=\"language-text\">part1.py</code>, including raw text analysis with CSV, text tokenizing, and word importance with a score called TF-IDF. The data file <code class=\"language-text\">raven.txt</code> is located in the <code class=\"language-text\">data.zip</code> file, so make sure to unzip it to <code class=\"language-text\">/data</code>! The remaining dataset files will be used for Part 2, so be sure to keep those handy.</p>\n<p><strong>TODO:</strong> Implement the incomplete stubs in <code class=\"language-text\">part1.py</code>.</p>\n<h2>Part 2: Classification with Sci-Kit Learn</h2>\n<blockquote>\n<p>Adapted from CIS 530 - Computational Linguistics</p>\n</blockquote>\n<h3>Preamble</h3>\n<p>The second part of the homework will be a longer project: building a text classifier. Now that we have seen tokenizing, text cleaning, and word importance with TF-IDF, let's train a text classifier that will be able to classify a word as being simple (e.g. <em>easy</em>, <em>act</em>, <em>blue</em>) or complex (e.g. <em>ostentatious</em>, <em>esoteric</em>, <em>aberration</em>). This is an important step in a larger NLP task to simply texts to make text more readable.</p>\n<p>In the provided code template with provided helper and unimplemented functions, you will need to:</p>\n<ol start=\"0\">\n<li>Look at the dataset! Try to understand the information that is conveyed to better understand the task.</li>\n<li>Implement the machine learning evaluation metric we discussed in class (accuracy).</li>\n<li>Perform data pre-processing for our dataset. You will need to parse the provided pre-labeled data in training/test sets, and implement a simple baseline model.</li>\n<li>Use the Sci-Kit Learn package to train machine learning models which classif\ny words as simple or complex.</li>\n</ol>\n<p>We have provided the dataset of labelled words split between training/test sets in (.txt) format. Some notes on the dataset:</p>\n<ol>\n<li>The training set is disjoint, so a word in <code class=\"language-text\">complex_words_training.txt</code> will not appear in the test set.</li>\n<li>Stop-words and proper nouns are already removed, leaving only nouns, verbs, and adjectives.</li>\n<li>There are 4,000 training words, and 1,000 testing words.</li>\n<li>The relevant columns are WORD (the word to be classified), and LABEL (0 for simple, 1 for complex).</li>\n</ol>\n<p>We have also provided frequencies (a contiguous sequence of 1 item from a given sample of text or speech) from the <a href=\"https://books.google.com/ngrams/info\">Google N-Gram Corpus</a>. This is to provide you another feature for classification. Consider why this extra information is useful for distinguishing between simple and complex words.</p>\n<p>Be sure to install <code class=\"language-text\">numpy</code> and <code class=\"language-text\">sklearn</code> before starting.</p>\n<h3>Section 0: Data (0 points)</h3>\n<p>We have provided the function <code class=\"language-text\">load_file</code> that takes in the file name <code class=\"language-text\">data_file</code> of one of the datasets and returns the words and labels of that dataset. The second provided helper function <code class=\"language-text\">load_ngram_counts</code> loads Google N-Gram counts from our provided file <code class=\"language-text\">ngram_counts.txt</code> as a dictionary of word frequencies.</p>\n<p><strong>TODO:</strong> Inspect these functions, print out what they return and make sure you understand what they're providing before moving on.</p>\n<h3>Section 1: Evaluation (5 points)</h3>\n<p>We will be implementing <strong>accuracy</strong>, a standard evaluation metric that we discussed in class. We will use this function later in the assignment, so be sure that this function works before moving on.</p>\n<p><strong>TODO:</strong> Implement <code class=\"language-text\">get_accuracy</code>, which should return a value between 0 and 1 that corresponds to the amount of predictions that match the true labels.</p>\n<h3>Section 2: Baseline Models (20 points)</h3>\n<p>In the following functions, you will implement 3 baseline models. Recall that baseline models are used to benchmark our own machine learning models against.</p>\n<ol>\n<li>The first baseline model <code class=\"language-text\">all_complex</code> classifies ALL words as complex (think back to the coin-flipping example from class).</li>\n<li>The second baseline model <code class=\"language-text\">word_length_threshold</code> uses word length thresholding: if a word is longer than the given threshold, we consider it to be complex, and vice versa.</li>\n<li>The third baseline model <code class=\"language-text\">word_frequency_threshold</code> is similar to the second, but we will use frequencies from the Google N-Gram counts dataset as the metric to threshold against.</li>\n</ol>\n<p><strong>TODO:</strong> Implement the three baseline models, and report their accuracies (using the function you implemented earlier).</p>\n<h3>Section 3: Machine Learning Models (20 points)</h3>\n<p>For our machine learning classifiers, we will use the built-in Naive Bayes and Logistic Regression models from Sci-Kit Learn to train a classifier. Refer to the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html\">Sci-Kit Learn Documentation</a> for more information about the Sci-Kit Learn API. Feel free to experiment with other models for extra credit!</p>\n<p>For features, you'll want to use both word length and word frequency. However, feel free to use any other features that you want! Be sure to document any extra features in <code class=\"language-text\">REPORT.md</code>. Extra credit is available for the inclusion of any other interesting features!</p>\n<p>You can import the relevant models from Sci-Kit Learn using:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>naive_bayes <span class=\"token keyword\">import</span> GaussianNB</code></pre></div>\n<p>for Naive Bayes and</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>linear_model <span class=\"token keyword\">import</span> LogisticRegression</code></pre></div>\n<p>for Logistic Regression. To train a classifier, you need two numpy arrays:</p>\n<ol>\n<li><code class=\"language-text\">X_train</code>: size (<code class=\"language-text\">m</code> x <code class=\"language-text\">n</code>), where <code class=\"language-text\">m</code> is the number of words and <code class=\"language-text\">n</code> is the number of features for each word.</li>\n<li><code class=\"language-text\">Y_train</code>: size (<code class=\"language-text\">m</code> x <code class=\"language-text\">1</code>) for the labels of each of the <code class=\"language-text\">m</code> words.</li>\n</ol>\n<p>Since Sci-Kit Learn classifiers take in <code class=\"language-text\">numpy</code> arrays, always wrap your lists in a <code class=\"language-text\">np.array</code>:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">my_array <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">,</span> <span class=\"token number\">4</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>As such, you'll want to import <code class=\"language-text\">numpy</code> using:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> numpy <span class=\"token keyword\">as</span> np</code></pre></div>\n<p><strong>TODO:</strong> Implement <code class=\"language-text\">logistic_regression</code> and <code class=\"language-text\">naive_bayes</code>, where you will train the machine learning models and report their accuracies on the testing data.</p>\n<h3>Section 4: Report (5 points)</h3>\n<p><strong>TODO:</strong> Complete <code class=\"language-text\">REPORT.md</code> with information about your implementations and your accuracies for each section. Write a few comments comparing the performace of your Naive Bayes classifier and your Logistic Regression classifier. Include any details about extra credit you've completed.</p>\n<p>Be sure to complete the report in <a href=\"https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet\">Markdown</a> format. Remember to indicate which member worked on which sections for full credit.</p>\n<h2>Submission</h2>\n<p>Submit all your code and potentially extra data to Gradescope. If you have a partner, YOU MUST MARK THEM AS A COLLABORATOR ON GRADESCOPE. If you fail to do this, you may get a 0 on this assignment.</p>\n<h2>Attributions</h2>\n<p><small> This homework was adapted from <a href=\"https://computational-linguistics-class.org/\">CIS 530: Computational Linguistics at the University of Pennsylvania</a> by Arun Kirubarajan and Kevin Sun.</small></p>"}},"pageContext":{"pathSlug":"/assignments/2"}},"staticQueryHashes":[]}